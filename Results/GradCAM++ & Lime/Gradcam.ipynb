{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 72\u001b[0m\n\u001b[0;32m     67\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m preprocess_image(\n\u001b[0;32m     68\u001b[0m     rgb_img, mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[0;32m     69\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Apply GradCAM++\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     74\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mapplyColorMap(\n\u001b[0;32m     75\u001b[0m     np\u001b[38;5;241m.\u001b[39muint8(\u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m grayscale_cam), cv2\u001b[38;5;241m.\u001b[39mCOLORMAP_JET\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m heatmap\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_grad_cam\\base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[0;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_grad_cam\\base_cam.py:74\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_input_gradient:\n\u001b[0;32m     71\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(input_tensor,\n\u001b[0;32m     72\u001b[0m                                            requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     target_categories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 19\u001b[0m, in \u001b[0;36mActivationsAndGradientsCustom.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Assuming the first element of the tuple is what we need\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[1;32mc:\\Users\\dunli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:24\u001b[0m, in \u001b[0;36mActivationsAndGradients.save_activation\u001b[1;34m(self, module, input, output)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_transform(activation)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39mappend(\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import preprocess_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Custom ActivationsAndGradients for handling tuple outputs\n",
    "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
    "\n",
    "class ActivationsAndGradientsCustom(ActivationsAndGradients):\n",
    "    def __init__(self, model, target_layers, reshape_transform=None):\n",
    "        super().__init__(model, target_layers, reshape_transform)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Forward pass\n",
    "        outputs = self.model(x)\n",
    "        \n",
    "        # Assuming the first element of the tuple is what we need\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "        \n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Initialize the model\n",
    "model = YOLO(r'C:\\Users\\dunli\\Documents\\STSY-project-main\\Training_Code\\runs\\detect\\train16\\weights\\best.pt')\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get the target layer for GradCAM\n",
    "target_layer = model.model.model[-1]\n",
    "\n",
    "# Directory containing the test images\n",
    "image_dir = r\"C:\\Users\\dunli\\Documents\\STSY-project-main\\Training Data\\test\\images\"\n",
    "\n",
    "# DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['Image', 'Class', 'Confidence', 'GradCAM_Path'])\n",
    "\n",
    "# Define the reshape transform function\n",
    "def reshape_transform(tensor):\n",
    "    # Flatten tensor for GradCAM\n",
    "    return tensor[0]\n",
    "\n",
    "# Initialize GradCAMPlusPlus with the reshape transform function\n",
    "cam = GradCAMPlusPlus(model=model.model, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n",
    "cam.activations_and_grads = ActivationsAndGradientsCustom(cam.model, cam.target_layers, reshape_transform)\n",
    "\n",
    "# Iterate over all images in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    \n",
    "    if not os.path.isfile(image_path):\n",
    "        continue\n",
    "\n",
    "    # Read and preprocess the image\n",
    "    rgb_img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if rgb_img is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "\n",
    "    input_tensor = preprocess_image(\n",
    "        rgb_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Apply GradCAM++\n",
    "    grayscale_cam = cam(input_tensor=input_tensor)[0]\n",
    "\n",
    "    heatmap = cv2.applyColorMap(\n",
    "        np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET\n",
    "    )\n",
    "    heatmap = heatmap.astype(np.float32) / 255\n",
    "\n",
    "    alpha = 0.3\n",
    "    final_output = cv2.addWeighted(rgb_img, alpha, heatmap, 1 - alpha, 0)\n",
    "\n",
    "    # Save the heatmap\n",
    "    heatmap_path = f\"gradcam_{os.path.basename(image_path)}\"\n",
    "    cv2.imwrite(heatmap_path, (final_output * 255).astype(np.uint8))\n",
    "\n",
    "    # Run YOLO inference\n",
    "    yolo_results = model(image_path)\n",
    "    for result in yolo_results:\n",
    "        for box in result.boxes:\n",
    "            class_name = box.cls\n",
    "            confidence = box.conf\n",
    "\n",
    "            # Append results to DataFrame\n",
    "            results_df = results_df.append({\n",
    "                'Image': image_path,\n",
    "                'Class': class_name,\n",
    "                'Confidence': confidence,\n",
    "                'GradCAM_Path': heatmap_path\n",
    "            }, ignore_index=True)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 8 rods, 2 microcolonys, 28.0ms\n",
      "Speed: 1.1ms preprocess, 28.0ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'xyxy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(img_tensor)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Iterate over results (assuming outputs is a list of Results objects)\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxyxy\u001b[49m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Extract bounding boxes, scores, and classes\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[:, :\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# Extract bounding boxes (x1, y1, x2, y2)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     scores \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[:, \u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# Extract confidence scores\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'xyxy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from yolo_cam.eigen_cam import EigenCAM\n",
    "from yolo_cam.utils.image import show_cam_on_image\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Replace with your actual paths and model initialization\n",
    "model_weights_path = r'C:\\Users\\dunli\\Documents\\STSY-project-main\\Training_Code\\runs\\detect\\train16\\weights\\best.pt'\n",
    "image_folder = r'C:\\Users\\dunli\\Documents\\STSY-project-main\\Training Data\\test\\images'\n",
    "\n",
    "# Initialize YOLO model\n",
    "model = YOLO(model_weights_path)\n",
    "\n",
    "# Resize dimensions (assuming YOLO model input size)\n",
    "resize_dim = (640, 640)\n",
    "\n",
    "# Initialize EigenCAM with the target layer\n",
    "target_layers = [model.model.model[-4]]  # Assuming -4 is the desired layer for EigenCAM\n",
    "cam = EigenCAM(model=model.model, target_layers=target_layers, task='od')\n",
    "\n",
    "# Iterate over each image in the directory\n",
    "for filename in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    \n",
    "    # Read and preprocess the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    rgb_img = cv2.resize(img, resize_dim)\n",
    "    img = np.float32(rgb_img) / 255.0\n",
    "    \n",
    "    # Convert image to tensor\n",
    "    img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).unsqueeze(0)  # assuming RGB image\n",
    "    \n",
    "    # Perform inference with YOLO\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "    \n",
    "    # Iterate over results (assuming outputs is a list of Results objects)\n",
    "    for result in outputs.xyxy:\n",
    "        # Extract bounding boxes, scores, and classes\n",
    "        boxes = result.cpu().numpy()[:, :4]  # Extract bounding boxes (x1, y1, x2, y2)\n",
    "        scores = result.cpu().numpy()[:, 4]  # Extract confidence scores\n",
    "        classes = result.cpu().numpy()[:, 5]  # Extract predicted classes\n",
    "        \n",
    "        # Iterate over each detected object\n",
    "        for box, score, cls in zip(boxes, scores, classes):\n",
    "            # Select the highest scoring box as the prediction\n",
    "            class_index = int(cls)  # Convert to int for class index\n",
    "            \n",
    "            # Apply EigenCAM to the predicted class\n",
    "            grayscale_cam = cam(img_tensor, class_index)[0, :, :]  # Compute EigenCAM for the image\n",
    "            \n",
    "            # Overlay EigenCAM on the original image\n",
    "            cam_image = show_cam_on_image(img, grayscale_cam.numpy(), use_rgb=True)\n",
    "            \n",
    "            # Display the result\n",
    "            plt.imshow(cam_image)\n",
    "            plt.title(f'EigenCAM: {filename} - Class {class_index}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
